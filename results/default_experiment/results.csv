,user_input,retrieved_contexts,response,reference,factual_correctness
0,What is an MoE?,"['Mixture of Experts (MoE), in the context of modern deep learning architectures, was proven effective in Shazeer et al. (2017). That work added an MoE layer which was stacked between LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately routed to combinations of experts. This resulted in state-of-the-art results in language modeling and machine translation benchmarks. The MoE layer was reintroduced into the Transformer architecture by the Mesh Tensorflow library (Shazeer et al., 2018)'
 'In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch']","Mixture of Experts (MoE) is a model in modern deep learning architectures that selects different parameters for each incoming example, resulting in a sparsely-activated model with a large number of parameters but a constant computational cost. It was proven effective in Shazeer et al. (2017) and reintroduced into the Transformer architecture by the Mesh Tensorflow library. Despite its successes, widespread adoption has been hindered by complexity, communication costs, and training instability.",MoE is something,0.0
1,What is an MoE?,"['Mixture of Experts (MoE), in the context of modern deep learning architectures, was proven effective in Shazeer et al. (2017). That work added an MoE layer which was stacked between LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately routed to combinations of experts. This resulted in state-of-the-art results in language modeling and machine translation benchmarks. The MoE layer was reintroduced into the Transformer architecture by the Mesh Tensorflow library (Shazeer et al., 2018)'
 'In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch']","Mixture of Experts (MoE) is a model in modern deep learning architectures that selects different parameters for each incoming example, resulting in a sparsely-activated model with a large number of parameters but a constant computational cost. It was proven effective in Shazeer et al. (2017) and reintroduced into the Transformer architecture by the Mesh Tensorflow library. Despite its successes, widespread adoption has been hindered by complexity, communication costs, and training instability.",MoE is something,0.0
