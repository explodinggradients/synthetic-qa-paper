{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-07-23T12:09:42.337401Z","iopub.status.busy":"2024-07-23T12:09:42.336717Z","iopub.status.idle":"2024-07-23T12:10:37.879309Z","shell.execute_reply":"2024-07-23T12:10:37.878172Z","shell.execute_reply.started":"2024-07-23T12:09:42.337368Z"},"id":"hrFYqiXsCPXD","outputId":"9d3caf44-b4e3-4686-e7a9-3d296bbce1ce","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c579fa440d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/langchain/\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/3c/78/c1de55eb3311f2c200a8b91724414b8d6f5ae78891c15d9d936ea43c3dba/marshmallow-3.22.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/typing-inspect/\u001b[0m\u001b[33m\n","\u001b[0m^C\n"]}],"source":["!pip install -q langchain langchain-core langchain-community langchain-huggingface ragatouille"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T12:16:19.741127Z","iopub.status.busy":"2024-07-23T12:16:19.740107Z","iopub.status.idle":"2024-07-23T12:16:23.617034Z","shell.execute_reply":"2024-07-23T12:16:23.615914Z","shell.execute_reply.started":"2024-07-23T12:16:19.741088Z"},"trusted":true},"outputs":[],"source":["%cd /content\n","!rm -rf card\n","!git clone https://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@github.com/ErfanMoosaviMonazzah/card.git\n","\n","%cd /content/card/src2"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-07-23T12:16:29.097980Z","iopub.status.busy":"2024-07-23T12:16:29.097568Z"},"id":"aCR1Kd-HHqQ7","outputId":"bdd95aa8-1213-4a5c-d27c-4dc2cf4b17de","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["- Loading docs from Cache: \"./caches/sample_experiment/cached_docs.pkl\"\n","- Loading Embedding Model & Tokenizer: thenlper/gte-small\n","- Chunk Size (#Tokens): 256\n","- Loading chunks from Cache: \"./caches/sample_experiment/cached_chunks.pkl\"\n","- Loading Vector DB from Cache: \"./caches/sample_experiment/cached_vector_db\"\n","- Loading Queries from Cache: \"./caches/sample_experiment/cached_queries.pkl\"\n","- Loading Rets from Cache: \"./caches/sample_experiment/cached_rets.pkl\"\n","- Loading Generator Model & Tokenizer: \"/home/pargar/hub/models/microsoft/Phi-3.5-mini-instruct\"\n","`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n","Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.38s/it]\n","/home/pargar/miniconda3/envs/tomcat_temp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/home/pargar/miniconda3/envs/tomcat_temp/lib/python3.11/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler()\n","- Loading reranked_rets from Cache: \"./caches/sample_experiment/cached_reranked_rets.pkl\"\n","  0%|                                                  | 0/2556 [00:00<?, ?it/s]/home/pargar/miniconda3/envs/tomcat_temp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","You are not running the flash-attention implementation, expect numerical differences.\n","  0%|▏                                        | 10/2556 [00:06<22:49,  1.86it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","100%|███████████████████████████████████████| 2556/2556 [23:27<00:00,  1.82it/s]\n","- Caching augmented_generations @ \"./caches/sample_experiment/cached_augmented_generations.pkl\"\n"]}],"source":["# !python card/src2/run_rag.py --yaml-config card/src2/sample.yaml --mode vrg\n","# took 23:27: 8toks, no batch\n","!python run_rag.py --yaml-config sample.yaml --mode vrg"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !zip -r files.zip card/src2/caches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assert False"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","import json, ret_evals\n","\n","with open('experiments/multihoprag_ret.json', 'r') as file:\n","    rets = json.load(file)['ret']\n","    \n","ds = load_dataset('yixuantt/MultiHopRAG', 'MultiHopRAG', split='train')\n","golds = [[gold['fact'] for gold in evs] for evs in ds['evidence_list']]\n","\n","ls_rets = []\n","ls_golds = []\n","\n","for row, ret, gold in zip(ds, rets, golds):\n","    if row['question_type'] == 'null_query':\n","        continue\n","    ls_rets.append(ret)\n","    ls_golds.append(gold)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Hits@4 0.3698: : 2255it [00:11, 191.61it/s]\n","Hits@10 0.5601: : 2255it [00:14, 159.10it/s]\n","MAP@10 0.1185: : 2255it [00:11, 204.60it/s]\n","MRR@10 0.2754: : 2255it [00:13, 165.81it/s]\n"]}],"source":["_ = ret_evals.hits_at(4, ls_rets, ls_golds)\n","_ = ret_evals.hits_at(10, ls_rets, ls_golds)\n","_ = ret_evals.map_at(10, ls_rets, ls_golds)\n","_ = ret_evals.mrr_at(10, ls_rets, ls_golds)"]},{"cell_type":"markdown","metadata":{},"source":["# multihoprag_gte"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["- Loading Corpus Dataset from HF: yixuantt/MultiHopRAG (corpus)\n","- Converting to LangChain Document: 100%|███| 609/609 [00:00<00:00, 5338.57it/s]\n","- Caching Documents at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_gte/cached_docs.pkl\"\n","- Loading Embedding Model & Tokenizer: thenlper/gte-small\n","model.safetensors: 100%|███████████████████| 66.7M/66.7M [00:22<00:00, 2.92MB/s]\n","tokenizer_config.json: 100%|███████████████████| 394/394 [00:00<00:00, 1.31MB/s]\n","vocab.txt: 100%|██████████████████████████████| 232k/232k [00:00<00:00, 652kB/s]\n","tokenizer.json: 100%|█████████████████████████| 712k/712k [00:00<00:00, 927kB/s]\n","special_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 429kB/s]\n","1_Pooling/config.json: 100%|████████████████████| 190/190 [00:00<00:00, 939kB/s]\n","- Chunk Size (#Tokens): 256\n","- Splitting Documents to Chunks: 100%|████████| 609/609 [00:24<00:00, 24.73it/s]\n","- Removing Duplicated Chunks: 100%|█████| 8193/8193 [00:00<00:00, 818247.32it/s]\n","- Caching Chunks at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_gte/cached_chunks.pkl\"\n","- 609 Documents splitted into 8,159 Chunks\n","- Vector DB: Start Embedding at 2024-07-24T12:34:40.147442\n","- Vector DB: Finished Embedding at 2024-07-24T12:36:06.159925\n","- Caching Vector DB at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_gte/cached_vector_db\"\n","- Loading Queries Dataset from HF: yixuantt/MultiHopRAG (MultiHopRAG)\n","- Caching Queries at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_gte/cached_queries.pkl\"\n","- Fetching Embeddings (2556 Queries)\n","- Searching Vector Index\n","- Fetching Similar Documents\n","- Saving Retrieved Documents: /home/erfan/Desktop/actives/card/src/experiments/multihoprag_gte_ret.json\n"]}],"source":["!python run_rag.py --yaml-config experiments/multihoprag_gte.yaml"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/erfan/miniconda3/envs/card_multihoprag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","import json, ret_evals\n","\n","with open('experiments/multihoprag_gte_ret.json', 'r') as file:\n","    rets = json.load(file)['ret']\n","    \n","ds = load_dataset('yixuantt/MultiHopRAG', 'MultiHopRAG', split='train')\n","golds = [[gold['fact'] for gold in evs] for evs in ds['evidence_list']]\n","\n","ls_rets = []\n","ls_golds = []\n","\n","for row, ret, gold in zip(ds, rets, golds):\n","    if row['question_type'] == 'null_query':\n","        continue\n","    ls_rets.append(ret)\n","    ls_golds.append(gold)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Hits@4 0.5401: : 2255it [00:06, 362.26it/s]\n","Hits@10 0.7144: : 2255it [00:06, 325.90it/s]\n","MAP@10 0.1949: : 2255it [00:06, 325.95it/s]\n","MRR@10 0.4136: : 2255it [00:07, 316.24it/s]\n"]}],"source":["_ = ret_evals.hits_at(4, ls_rets, ls_golds)\n","_ = ret_evals.hits_at(10, ls_rets, ls_golds)\n","_ = ret_evals.map_at(10, ls_rets, ls_golds)\n","_ = ret_evals.mrr_at(10, ls_rets, ls_golds)"]},{"cell_type":"markdown","metadata":{},"source":["# multihoprag_bge"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["- Loading Documents from Cache: \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_bge/cached_docs.pkl\"\n","- Loading Embedding Model & Tokenizer: BAAI/bge-large-en-v1.5\n","tokenizer_config.json: 100%|████████████████████| 366/366 [00:00<00:00, 656kB/s]\n","vocab.txt: 100%|██████████████████████████████| 232k/232k [00:00<00:00, 387kB/s]\n","tokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 1.06MB/s]\n","special_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 436kB/s]\n","1_Pooling/config.json: 100%|████████████████████| 191/191 [00:00<00:00, 424kB/s]\n","- Chunk Size (#Tokens): 256\n","- Splitting Documents to Chunks:   0%|                  | 0/609 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n","- Splitting Documents to Chunks: 100%|████████| 609/609 [00:25<00:00, 24.01it/s]\n","- Removing Duplicated Chunks: 100%|████| 8193/8193 [00:00<00:00, 1177088.88it/s]\n","- Caching Chunks at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_bge/cached_chunks.pkl\"\n","- 609 Documents splitted into 8,159 Chunks\n","- Vector DB: Start Embedding at 2024-07-24T12:58:27.145320\n","- Vector DB: Finished Embedding at 2024-07-24T13:10:08.573201\n","- Caching Vector DB at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_bge/cached_vector_db\"\n","- Loading Queries Dataset from HF: yixuantt/MultiHopRAG (MultiHopRAG)\n","- Caching Queries at \"/home/erfan/Desktop/actives/card/src/experiments/multihoprag_bge/cached_queries.pkl\"\n","- Fetching Embeddings (2556 Queries)\n","- Searching Vector Index\n","- Fetching Similar Documents\n","- Saving Retrieved Documents: /home/erfan/Desktop/actives/card/src/experiments/multihoprag_bge_ret.json\n"]}],"source":["!python run_rag.py --yaml-config experiments/multihoprag_bge.yaml"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","import json, ret_evals\n","\n","with open('experiments/multihoprag_bge_ret.json', 'r') as file:\n","    rets = json.load(file)['ret']\n","    \n","ds = load_dataset('yixuantt/MultiHopRAG', 'MultiHopRAG', split='train')\n","golds = [[gold['fact'] for gold in evs] for evs in ds['evidence_list']]\n","\n","ls_rets = []\n","ls_golds = []\n","\n","for row, ret, gold in zip(ds, rets, golds):\n","    if row['question_type'] == 'null_query':\n","        continue\n","    ls_rets.append(ret)\n","    ls_golds.append(gold)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Hits@4 0.5871: : 2255it [00:02, 1005.75it/s]\n","Hits@10 0.7508: : 2255it [00:02, 1127.07it/s]\n","MAP@10 0.2121: : 2255it [00:02, 1059.75it/s]\n","MRR@10 0.4501: : 2255it [00:02, 1063.97it/s]\n"]}],"source":["_ = ret_evals.hits_at(4, ls_rets, ls_golds)\n","_ = ret_evals.hits_at(10, ls_rets, ls_golds)\n","_ = ret_evals.map_at(10, ls_rets, ls_golds)\n","_ = ret_evals.mrr_at(10, ls_rets, ls_golds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [5:25 PM] Luis Espinosa-Anke\n","# import numpy as np\n"," \n","# # Assuming questionsdb and vectordb are already defined and are numpy arrays\n","\n","# res = questionsdb @ vectordb.T\n"," \n","# # Flatten the result to a 1D array\n","\n","# flattened_res = res.flatten()\n"," \n","# # Get the indices of the top 10 most similar vectors\n","\n","# top_10_indices = np.argsort(flattened_res)[-10:][::-1]\n"," \n","# # Retrieve the top 10 most similar vectors\n","\n","# top_10_similar = flattened_res[top_10_indices]\n"," \n","# print(\"Indices of top 10 most similar vectors:\", top_10_indices)\n","\n","# print(\"Top 10 most similar vectors:\", top_10_similar)\n"," \n","# [5:25 PM] Luis Espinosa-Anke\n","# vectordb = rag.vectorizer.vector_db.index.reconstruct_n(0)\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
